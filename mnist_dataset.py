# -*- coding: utf-8 -*-
"""MNIST Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aB76s1BbvkcDXN-sms-xGQWLqw2A57US

Programa destinado à classificação das imagens dos dígitos manuscritos do dataset MNIST
"""

!pip install tensorflow keras numpy mnist matplotlib

"""Importação dos pacotes/dependências"""

import numpy as np
import mnist #buscando o dataset utilizado
import matplotlib.pyplot as plt #gráficos
from keras.models import Sequential #arquitetura de redes neurais
from keras.layers import Dense #camadas de uma rede neural
from keras.utils import to_categorical

"""Carregamento do data set"""

imgTreino = mnist.train_images() #imagens p treino
labelTreino = mnist.train_labels() #etiquetas p treino
imgTeste = mnist.test_images()
labelTeste = mnist.test_labels()

"""Normalizando as imagens: arrumando os valores do range 0 à 255 para o range -0.5 à 0.5, com o intuito de fazer a rede mais fácil de ser treinada"""

imgTreino = (imgTreino/255) - 0.5 #essa conta se dá devido ao fato de que a variável imgTreino dividido por 255 pode ter como resultado 1 ou 0
#Esses dois valores subtraindo 0.5 faz com que as imagens cheguem no range que a gente deseja
imgTeste = (imgTeste/255) - 0.5
#"achatamento" (flattening) nas imagens. isso fará com que cada imagem 28x28 se transforme em um vetor dimensional 784 (28^2) para que seja passado para a RN
imgTreino = imgTreino.reshape((-1, 784))
imgTeste = imgTeste.reshape((-1, 784))
#print no nosso shape das imagens
print(imgTreino.shape) #60000 linhas e 784 colunas
print(imgTeste.shape) #10000 linhas e 784 colunas

"""Construindo o modelo: 3 camadas, sendo 2 com 16 neurônios e função relu e uma delas com 5 neurônios e função softmax"""

modelo = Sequential()
modelo.add (Dense(64, activation = 'relu', input_dim= 784))
modelo.add (Dense(64, activation = 'relu'))
modelo.add (Dense(10, activation = 'softmax'))

"""Compilando o modelo: a função de perda mede o quão bem o modelo irá no treinamento e tentará melhorar nele usando o otimizador"""

modelo.compile(
    optimizer='adam',
      loss = 'categorical_crossentropy', 
      metrics = ['accuracy']
)

"""Treinando o modelo"""

modelo.fit(
  imgTreino,
    to_categorical(labelTreino), #Ex. retorna 2 e espera [0, 0, 0, 0, 1, 0, 0, 0, 0, 0] = converte o valor para um vetor dimensional de 10
    epochs = 5, #o numero de iterações que acontecerão durante o treino do dataset
    batch_size=32 #numero de amostras por atualização gradiente de treino
)

"""Avaliação do modelo"""

modelo.evaluate(
    imgTeste,
      to_categorical(labelTeste),

)

"""model.save_weights('modelo.h5') <= para salvar o modelo, caso queira

previsão nas primeiras 5 imagens de teste
"""

previsao = modelo.predict(imgTeste[:5])
print(np.argmax(previsao, axis = 1))
print(labelTeste[:5])

for i in range(0,5):
  imgUm = imgTeste[i]
  imgUm = np.array(imgUm, dtype= 'float')
  pixels = imgUm.reshape((28,28))
  plt.imshow(pixels, cmap= 'gray')
  plt.show()